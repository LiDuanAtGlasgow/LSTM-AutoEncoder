[[0.43396226 0.13207547 0.11320755 0.0754717  0.2264151 ]
 [0.03597122 0.41007194 0.36690646 0.07194245 0.10791367]
 [0.         0.02955665 0.7881773  0.06403941 0.11330049]
 [0.         0.00858369 0.03004292 0.8111588  0.14592275]
 [0.01061008 0.01856764 0.0530504  0.05570292 0.8594164 ]]
/home/kentuen/AutoEncoder-LSTM/LSTM-AutoEncoder/Train/classification.py:286: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  sbplt1=plt.subplot()
Train Loss is: [0.9373167965114116, 0.7034988677948714, 0.599232095927, 0.47488914816826583, 0.3354201698899269, 0.3016000317633152, 0.27544896272569896, 0.2530677242875099, 0.2312282273322344, 0.23452377033233643, 0.23560175566375255, 0.2372879340350628, 0.23182204857468605, 0.23518287794291973, 0.23503919788450003, 0.22969046764820814, 0.2364728932455182, 0.23541217268258333, 0.23342305088043214, 0.24076876047998666, 0.2429040074273944, 0.24017128820717334]
Train acc is: [63.949999999999996, 72.91250000000001, 77.05, 82.7125, 89.075, 90.47500000000001, 91.5875, 92.8875, 93.86250000000001, 93.89999999999999, 93.625, 93.86250000000001, 94.325, 94.175, 94.28750000000001, 94.5375, 94.5125, 94.39999999999999, 94.77499999999999, 94.475, 94.55, 94.72500000000001]
Test Loss is: [0.8963834630250931, 0.6747957760095596, 0.6679160596132279, 0.6123678764104843, 0.6056620094776154, 0.534644336104393, 0.5184663090705871, 0.4997954130172729, 0.5358195647001266, 0.5108976182937622, 0.5158596421480179, 0.5736182239055634, 0.5471015996932983, 0.5509198248386383, 0.534521181166172, 0.5688881472349167, 0.5921303215026855, 0.5761112744808197, 0.5733541645407677, 0.5669426237940788, 0.5660237218141556, 0.5649775173664093]
Test acc is: [59.699999999999996, 75.0, 75.2, 77.2, 79.7, 83.39999999999999, 82.19999999999999, 83.3, 81.89999999999999, 80.2, 80.7, 76.7, 78.7, 78.9, 79.7, 78.10000000000001, 78.7, 79.4, 78.9, 79.2, 78.7, 77.10000000000001]

[[0.36619717 0.2112676  0.         0.04225352 0.36619717]
 [0.01470588 0.7352941  0.05147059 0.06617647 0.125     ]
 [0.         0.         0.76958525 0.01843318 0.20737328]
 [0.         0.0173913  0.04782609 0.90434784 0.02608696]
 [0.00569801 0.         0.01994302 0.03133903 0.94017094]]
/home/kentuen/AutoEncoder-LSTM/LSTM-AutoEncoder/Train/classification.py:286: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  sbplt1=plt.subplot()
Train Loss is: [0.9612376518025995, 0.5460436071902514, 0.36645431150123475, 0.265019424745813, 0.18144718263670803, 0.17721470970660447, 0.17292467093467712, 0.16678609203919767, 0.15840868838317693, 0.1563377926107496, 0.141801778960973, 0.14358200075291097, 0.1499865759126842, 0.15549489439092576, 0.1611752291433513, 0.15769455735757948, 0.1622963232472539, 0.1589438234064728, 0.15857617688551545, 0.154008950734511, 0.1487236139215529, 0.14280252299830318]
Train acc is: [63.5125, 80.4875, 87.4375, 90.5875, 93.63749999999999, 93.7125, 94.16250000000001, 94.0, 94.0625, 94.1875, 94.6875, 94.7625, 94.5875, 94.19999999999999, 93.75, 93.72500000000001, 93.21249999999999, 93.21249999999999, 92.975, 93.4125, 94.1375, 94.525]
Test Loss is: [0.7448292366862297, 0.6749053590297699, 0.44125395077466967, 0.47258207070827485, 0.3792645817101002, 0.3703899085521698, 0.31429117000102996, 0.2679520864486694, 0.2857398766577244, 0.2918571679294109, 0.2905194485038519, 0.2844444787502289, 0.30658420702815053, 0.33046100521087646, 0.35138268214464186, 0.3325417072921991, 0.2875271568596363, 0.2765365092158318, 0.2854744674414396, 0.2783972924351692, 0.2860419184565544, 0.28691699957847594]
Test acc is: [76.4, 77.10000000000001, 84.8, 85.6, 85.8, 86.7, 90.0, 89.4, 89.3, 89.7, 88.7, 87.7, 87.1, 85.7, 85.3, 84.8, 86.4, 87.0, 86.6, 86.3, 84.6, 84.0]

[[0.15337424 0.19631901 0.17791411 0.18404908 0.2822086 ]
 [0.20867209 0.09214092 0.11382114 0.00813008 0.5745258 ]
 [0.         0.02251407 0.4577861  0.14258912 0.3752345 ]
 [0.         0.02228826 0.05052006 0.51411587 0.4115899 ]
 [0.12385919 0.05215124 0.09647979 0.02477184 0.70143414]]
/home/kentuen/AutoEncoder-LSTM/LSTM-AutoEncoder/Train/LOOP_train.py:268: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  sbplt1=plt.subplot()
Train Loss is: [1.0590143009541082, 0.7529484506962346, 0.6208713839568344, 0.4967905487827226, 0.3941126244208392, 0.3951785846504511, 0.3730946536438138, 0.3474124018725227, 0.3325992125389623, 0.32510775416505105, 0.3227780438778447, 0.3114311572336683, 0.3050603005699083, 0.29205135543673644, 0.2937856302915835, 0.29784939659810533, 0.30270587623820583, 0.30493616241567273, 0.3055133492993373, 0.3044991800083834, 0.3221432201722089, 0.3263838643466725]
Train acc is: [58.60392156862745, 70.27450980392157, 75.67058823529412, 82.18039215686275, 86.25882352941177, 86.7921568627451, 87.27843137254901, 88.32941176470588, 89.06666666666668, 89.38039215686274, 89.38039215686274, 89.86666666666666, 90.18039215686274, 90.52549019607842, 90.65098039215687, 90.7921568627451, 91.13725490196079, 90.7764705882353, 90.55686274509804, 90.8235294117647, 90.25882352941177, 89.77254901960784]
Test Loss is: [0.8898149870766534, 0.744546697722541, 0.7601806406974793, 0.7718306127654182, 0.6902026672363282, 0.6985638882584042, 0.7590190787050459, 0.7154967816140917, 0.7029959644211663, 0.7080000947316487, 0.6844526894887288, 0.6422897943390741, 0.6250846536954244, 0.6103867596520318, 0.5938280048105452, 0.5657784291373359, 0.5369070671399434, 0.543142650047938, 0.5588113420274523, 0.5533530246416728, 0.5226034819814894, 0.46396739318635727]
Test acc is: [66.04444444444444, 70.66666666666667, 69.95555555555556, 73.24444444444444, 77.33333333333333, 75.73333333333333, 71.46666666666667, 73.95555555555555, 73.24444444444444, 75.1111111111111, 75.46666666666667, 76.62222222222222, 76.26666666666667, 78.13333333333333, 79.46666666666667, 80.44444444444444, 81.5111111111111, 82.39999999999999, 81.42222222222222, 82.93333333333334, 83.11111111111111, 85.51111111111112]

[[0.20689656 0.21551724 0.14655173 0.18103448 0.2413793 ]
 [0.24691358 0.20987654 0.10185185 0.02160494 0.41666666]
 [0.         0.03207547 0.49245283 0.13018867 0.34339622]
 [0.         0.00285307 0.04422254 0.5406562  0.41084164]
 [0.13549161 0.01918465 0.05155875 0.06594724 0.7266187 ]]
/home/kentuen/AutoEncoder-LSTM/LSTM-AutoEncoder/Train/LOOP_train.py:268: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  sbplt1=plt.subplot()
Train Loss is: [0.7577127033121446, 0.3993669730354758, 0.219764754893733, 0.11699526496962005, 0.03487973106608671, 0.0255903242522595, 0.01921218905729406, 0.013493904450360467, 0.009415848189709234, 0.009171998229681276, 0.00886234821992762, 0.008353751126457663, 0.007746137656417547, 0.007498566833196902, 0.007521197300331265, 0.008135864781398399, 0.007877755707385494, 0.007506697262034697, 0.006910391265270757, 0.006686284308340036, 0.006501963970707912, 0.005970873290417241]
Train acc is: [70.33725490196079, 85.03529411764706, 92.23529411764706, 96.14117647058823, 99.63921568627451, 99.82745098039216, 99.89019607843137, 99.90588235294118, 99.96862745098039, 99.93725490196078, 99.95294117647059, 99.9843137254902, 99.9843137254902, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
Test Loss is: [0.6810216156641642, 0.5420674256218804, 0.5308636526531644, 0.5875281821356879, 0.4484871548546685, 0.5078242984347874, 0.4832073908911811, 0.5687664150661892, 0.5661759656270345, 0.5997696478101943, 0.646196278889974, 0.4830647737714979, 0.47106125937567817, 0.4811148749457465, 0.3888997836642795, 0.41928772248162166, 0.4111154857211643, 0.3716919008890788, 0.3386149196624756, 0.42316355938381617, 0.404760986328125, 0.3678361937204997]
Test acc is: [73.77777777777777, 83.11111111111111, 82.66666666666667, 80.26666666666667, 86.66666666666667, 87.28888888888889, 86.93333333333332, 85.24444444444444, 83.91111111111111, 82.84444444444445, 81.5111111111111, 84.26666666666667, 83.91111111111111, 84.26666666666667, 86.66666666666667, 86.4, 86.4888888888889, 86.4, 88.08888888888889, 85.15555555555555, 85.6, 86.84444444444445]
Train Ends!

[[5.5761319e-01 1.2139918e-01 6.7901231e-02 2.0576132e-02 2.3045267e-01]
 [7.4163091e-01 2.1974249e-01 0.0000000e+00 0.0000000e+00 3.7768241e-02]
 [3.1272402e-01 1.9758065e-01 3.9112905e-01 3.5842296e-02 6.2275987e-02]
 [1.1046699e-01 2.6892111e-01 5.9903383e-02 2.8856683e-01 2.7181965e-01]
 [7.1880752e-01 6.6617593e-02 4.7110785e-02 3.6805301e-04 1.6672800e-01]]
22
22
22
22
22
22
22
/home/kentuen/AutoEncoder-LSTM/LSTM-AutoEncoder/Train/LOOP_train.py:323: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  sbplt1=plt.subplot()
Class Train Loss is: [2.3407566133515325, 2.2663038309369097, 2.155006686152564, 1.8832024473376905, 1.9324148871106994, 1.7734268530179136, 1.4726232622087616, 1.4639832217873576, 1.3533162584957183, 1.2553706863696632, 1.1729413929008126, 1.0945258754327944, 1.0853905618293178, 1.0134440452372495, 1.0476198436545137, 1.0713115135137363, 1.0600358679352961, 1.0330759923277852, 0.9706714892049426, 0.9762030386740399, 0.9362891812299945, 0.8991339871359035]
Class Train acc is: [13.25171821305842, 19.282646048109967, 24.531786941580755, 30.884879725085913, 31.22422680412371, 34.982817869415804, 43.55670103092783, 44.03780068728522, 49.05068728522337, 52.02319587628866, 52.72766323024055, 56.34879725085911, 57.86082474226804, 60.45103092783505, 58.36340206185567, 58.01546391752578, 57.78350515463917, 58.1872852233677, 61.18127147766324, 61.4905498281787, 62.80927835051546, 64.43298969072166]
Class Val Loss is: [2.436291355713946, 1.531034088913108, 1.4958180532422671, 1.462896720097237, 1.4998756142211533, 1.0542693222408852, 1.2045233099423733, 1.3669372464168523, 1.1648992762635253, 1.0830455607155345, 1.1171285908758846, 1.134943330124072, 0.9582983624382118, 0.9663901916484243, 1.0666247936356108, 1.0747428795418787, 0.8780221832576895, 0.9956926817746506, 1.0737420653149845, 1.1213226927719575, 0.9094365065245285, 0.905879038500622]
Class Val acc is: [13.556701030927835, 29.879725085910653, 33.12714776632302, 41.52920962199313, 30.36082474226804, 52.78350515463918, 42.81786941580756, 42.18213058419244, 46.54639175257732, 52.57731958762887, 50.618556701030926, 51.71821305841925, 58.333333333333336, 57.02749140893471, 52.21649484536083, 51.92439862542956, 65.18900343642612, 57.09621993127148, 55.49828178694158, 51.99312714776633, 61.185567010309285, 63.90034364261168]
Memo Val Loss is: [1086.78463744855, 744.7591406989343, 643.5502019639687, 581.1089277850804, 504.1230008679157, 468.23085803395696, 437.74832521287846, 407.17499428713035, 379.9642748239524, 358.35365659903823, 344.36955260771657, 333.0034344191404, 318.4494060765427, 306.85803985464617, 297.8597445635451, 287.063556714402, 273.84378161676153, 271.2012282158501, 261.5742789055474, 256.24659824043613, 252.88320013813137, 248.92656169576742]
Memo Train Loss is: [1074.6631483254973, 862.5353851318359, 745.5210389022565, 846.240007213055, 759.8142404143343, 747.3225096030743, 687.7301988214971, 712.2630461335592, 684.4326740592616, 697.3188786195316, 706.460370187661, 673.0082502070162, 697.0240011733013, 683.7722746124792, 720.3523225450024, 704.7484850067453, 739.3213168848831, 712.0377365374483, 739.917225447717, 724.5188277464142, 747.9779132803691, 753.0614018076474]
