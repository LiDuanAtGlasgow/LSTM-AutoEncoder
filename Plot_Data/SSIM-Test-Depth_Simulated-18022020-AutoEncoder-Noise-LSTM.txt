train loss is: [0.8291190330952536, 0.8533729907786893, 0.8609099656417214, 0.867333045893457, 0.8738408497480263, 0.8780670256250908, 0.8798899336154407, 0.881105851320319, 0.8820356505327445, 0.8827499054753846, 0.8832753302964532, 0.8837543846498597, 0.8841359957119362, 0.8844564476456616, 0.8847463232873395, 0.88497873538509, 0.885166312898542, 0.8853905377797846, 0.8855799893078612, 0.8858389395964434, 0.8861754811759814, 0.8864885557555662, 0.8868252957165639, 0.8870942864546475, 0.8872532458667776, 0.8873833738212106, 0.8874895311658005, 0.8876493415553616, 0.8877788291371438, 0.887881511759581, 0.887982765663335, 0.8880653884038456, 0.8881659467980995, 0.8882360477039479, 0.8882739688047291, 0.8883440643917736]
test loss is: [0.8464049833651023, 0.8558206296457282, 0.8628770293632099, 0.8688375479195111, 0.8752672643337435, 0.8783122988489726, 0.8800580164443509, 0.8806136955811867, 0.8813145437876928, 0.8826123686170841, 0.8831922210513263, 0.8835421813435267, 0.8846550216657557, 0.884365954275202, 0.883824923164205, 0.8850597612476113, 0.8853380973852409, 0.885091548541966, 0.8858562935857848, 0.8860680020107398, 0.8861665666704742, 0.8866042088786013, 0.886717203547109, 0.8871321267861991, 0.8871839990643985, 0.8854834363616219, 0.8869989737655557, 0.8877014641350572, 0.887431315925946, 0.8880754927442388, 0.8878492746334087, 0.8880328923639697, 0.8879562675011129, 0.8881233125874004, 0.8884892848226517, 0.8884125968149542]